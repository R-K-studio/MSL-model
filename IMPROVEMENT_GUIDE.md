# æ‰‹è¯­è¯†åˆ«æ¨¡å‹å‡†ç¡®ç‡æå‡æŒ‡å—

## ğŸ“Š å½“å‰çŠ¶æ€åˆ†æ

**è®­ç»ƒç»“æœ**:
- è®­ç»ƒå‡†ç¡®ç‡: 91.09%
- æµ‹è¯•å‡†ç¡®ç‡: 74.41%
- **è¿‡æ‹Ÿåˆç¨‹åº¦**: 16.68% (è®­ç»ƒå‡†ç¡®ç‡ - æµ‹è¯•å‡†ç¡®ç‡)

**é—®é¢˜è¯Šæ–­**:
1. âœ… **ä¸¥é‡è¿‡æ‹Ÿåˆ**: è®­ç»ƒå‡†ç¡®ç‡è¿œé«˜äºæµ‹è¯•å‡†ç¡®ç‡
2. âš ï¸ **æ³›åŒ–èƒ½åŠ›ä¸è¶³**: æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°æ˜æ˜¾ä¸‹é™
3. ğŸ“‰ **å‡†ç¡®ç‡æœ‰æå‡ç©ºé—´**: 74%å¯¹äº90ä¸ªç±»åˆ«è¿˜æœ‰æ”¹è¿›ä½™åœ°

---

## ğŸ¯ æå‡ç­–ç•¥ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰

### 1. è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜ â­â­â­â­â­

#### 1.1 å¢åŠ æ•°æ®å¢å¼º

**é—®é¢˜**: è®­ç»ƒæ•°æ®å¯èƒ½ä¸è¶³æˆ–å¤šæ ·æ€§ä¸å¤Ÿ

**è§£å†³æ–¹æ¡ˆ**: åœ¨ `sign_language_recognition.py` ä¸­æ·»åŠ æ•°æ®å¢å¼º

```python
# åœ¨ SignLanguageTrainer ç±»ä¸­æ·»åŠ æ•°æ®å¢å¼ºæ–¹æ³•
def augment_sequence(self, sequence):
    """æ•°æ®å¢å¼ºï¼šæ·»åŠ å™ªå£°ã€æ—¶é—´æ‰­æ›²ç­‰"""
    augmented = sequence.copy()
    
    # 1. æ·»åŠ é«˜æ–¯å™ªå£°
    noise = np.random.normal(0, 0.01, sequence.shape)
    augmented = augmented + noise
    
    # 2. æ—¶é—´æ‰­æ›²ï¼ˆéšæœºè·³è¿‡æˆ–é‡å¤å¸§ï¼‰
    if np.random.random() > 0.5:
        indices = np.random.choice(len(sequence), size=len(sequence), replace=True)
        augmented = augmented[indices]
    
    # 3. ç¼©æ”¾å…³é”®ç‚¹ï¼ˆæ¨¡æ‹Ÿä¸åŒè·ç¦»ï¼‰
    scale = np.random.uniform(0.95, 1.05)
    augmented = augmented * scale
    
    return augmented
```

**ä¿®æ”¹è®­ç»ƒå¾ªç¯**:
```python
# åœ¨ train() æ–¹æ³•ä¸­ï¼Œè®­ç»ƒæ—¶å¯¹æ•°æ®è¿›è¡Œå¢å¼º
for sequences_batch, labels_batch in train_loader:
    # æ•°æ®å¢å¼ºï¼ˆä»…è®­ç»ƒæ—¶ï¼‰
    if self.model.training:
        augmented_batch = []
        for seq in sequences_batch:
            augmented_batch.append(self.augment_sequence(seq.cpu().numpy()))
        sequences_batch = torch.FloatTensor(augmented_batch).to(self.device)
    else:
        sequences_batch = sequences_batch.to(self.device)
    # ... ç»§ç»­è®­ç»ƒ
```

#### 1.2 å¢åŠ æ­£åˆ™åŒ–

**ä¿®æ”¹æ¨¡å‹æ¶æ„**ï¼Œå¢åŠ æ›´å¼ºçš„æ­£åˆ™åŒ–ï¼š

```python
class SignLanguageLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes, dropout_rate=0.5):
        super(SignLanguageLSTM, self).__init__()
        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True, num_layers=1, dropout=dropout_rate)
        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True, num_layers=1, dropout=dropout_rate)
        self.lstm3 = nn.LSTM(hidden_size, hidden_size, batch_first=True, num_layers=1, dropout=dropout_rate)
        
        # å¢åŠ Dropoutå±‚
        self.dropout1 = nn.Dropout(dropout_rate)
        self.dropout2 = nn.Dropout(dropout_rate)
        self.dropout3 = nn.Dropout(dropout_rate)
        
        self.fc1 = nn.Linear(hidden_size, 64)
        self.fc2 = nn.Linear(64, 128)
        self.fc3 = nn.Linear(128, 64)
        self.fc4 = nn.Linear(64, 32)
        self.fc5 = nn.Linear(32, 32)
        self.output_layer = nn.Linear(32, num_classes)
        
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x, _ = self.lstm1(x)
        x = self.dropout1(x)
        x, _ = self.lstm2(x)
        x = self.dropout2(x)
        x, _ = self.lstm3(x)
        x = self.dropout3(x)
        
        x = x[:, -1, :]
        
        x = self.relu(self.fc1(x))
        x = self.dropout1(x)
        x = self.relu(self.fc2(x))
        x = self.dropout2(x)
        x = self.relu(self.fc3(x))
        x = self.dropout3(x)
        x = self.relu(self.fc4(x))
        x = self.relu(self.fc5(x))
        x = self.output_layer(x)
        
        return x
```

**ä¿®æ”¹CONFIG**:
```python
CONFIG = {
    # ... å…¶ä»–é…ç½®
    'dropout_rate': 0.5,  # å¢åŠ dropoutç‡
    'weight_decay': 0.0001,  # L2æ­£åˆ™åŒ–
}
```

**ä¿®æ”¹ä¼˜åŒ–å™¨**:
```python
optimizer = optim.Adam(
    self.model.parameters(), 
    lr=self.config['learning_rate'],
    weight_decay=self.config.get('weight_decay', 0.0001)  # æ·»åŠ æƒé‡è¡°å‡
)
```

#### 1.3 ä½¿ç”¨Early Stopping

**æ·»åŠ æ—©åœæœºåˆ¶**ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼š

```python
def train(self, sequences, labels):
    # ... å‰é¢çš„ä»£ç  ...
    
    best_test_acc = 0.0
    patience = 20  # å¦‚æœ20ä¸ªepochæ²¡æœ‰æå‡å°±åœæ­¢
    patience_counter = 0
    
    for epoch in range(self.config['num_epochs']):
        # ... è®­ç»ƒä»£ç  ...
        
        test_acc = self.evaluate(test_loader)
        test_accuracies.append(test_acc)
        
        # Early stopping
        if test_acc > best_test_acc:
            best_test_acc = test_acc
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save({...}, self.config['model_save_path'])
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"Early stopping at epoch {epoch+1}")
                break
        
        # ... å…¶ä»–ä»£ç  ...
```

---

### 2. ä¼˜åŒ–æ¨¡å‹æ¶æ„ â­â­â­â­

#### 2.1 ä½¿ç”¨åŒå‘LSTM

åŒå‘LSTMå¯ä»¥æ•è·å‰åæ–‡ä¿¡æ¯ï¼š

```python
self.lstm1 = nn.LSTM(
    input_size, hidden_size, 
    batch_first=True, 
    num_layers=1,
    bidirectional=True,  # åŒå‘
    dropout=dropout_rate
)
# æ³¨æ„ï¼šåŒå‘LSTMè¾“å‡ºç»´åº¦æ˜¯ hidden_size * 2
self.fc1 = nn.Linear(hidden_size * 2, 64)  # éœ€è¦è°ƒæ•´
```

#### 2.2 ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶

æ·»åŠ æ³¨æ„åŠ›å±‚å…³æ³¨é‡è¦å¸§ï¼š

```python
class AttentionLayer(nn.Module):
    def __init__(self, hidden_size):
        super(AttentionLayer, self).__init__()
        self.attention = nn.Linear(hidden_size, 1)
    
    def forward(self, lstm_output):
        # lstm_output: (batch, seq_len, hidden_size)
        attention_weights = torch.softmax(self.attention(lstm_output), dim=1)
        attended_output = torch.sum(attention_weights * lstm_output, dim=1)
        return attended_output

# åœ¨æ¨¡å‹ä¸­æ·»åŠ 
self.attention = AttentionLayer(hidden_size)
# åœ¨forwardä¸­ä½¿ç”¨
x = self.lstm3(x)
x = self.attention(x)  # ä½¿ç”¨æ³¨æ„åŠ›è€Œä¸æ˜¯ç›´æ¥å–æœ€åä¸€å¸§
```

#### 2.3 å¢åŠ æ¨¡å‹å®¹é‡ï¼ˆå¦‚æœæ•°æ®è¶³å¤Ÿï¼‰

```python
CONFIG = {
    'hidden_size': 128,  # ä»64å¢åŠ åˆ°128
    'num_layers': 2,     # LSTMå±‚æ•°
}
```

---

### 3. ä¼˜åŒ–è®­ç»ƒç­–ç•¥ â­â­â­

#### 3.1 å­¦ä¹ ç‡è°ƒåº¦

ä½¿ç”¨å­¦ä¹ ç‡è¡°å‡ï¼š

```python
# åœ¨train()æ–¹æ³•ä¸­
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, 
    mode='max',  # ç›‘æ§æµ‹è¯•å‡†ç¡®ç‡
    factor=0.5,  # æ¯æ¬¡å‡å°‘ä¸€åŠ
    patience=10,  # 10ä¸ªepochæ²¡æœ‰æå‡å°±é™ä½
    verbose=True
)

# åœ¨æ¯ä¸ªepochå
scheduler.step(test_acc)
```

#### 3.2 ä½¿ç”¨ä¸åŒçš„ä¼˜åŒ–å™¨

å°è¯•AdamWæˆ–SGDï¼š

```python
# AdamW (æ›´å¥½çš„æƒé‡è¡°å‡)
optimizer = optim.AdamW(
    self.model.parameters(),
    lr=self.config['learning_rate'],
    weight_decay=0.01
)

# æˆ–SGD with momentum
optimizer = optim.SGD(
    self.model.parameters(),
    lr=0.01,
    momentum=0.9,
    weight_decay=0.0001
)
```

#### 3.3 è°ƒæ•´æ‰¹æ¬¡å¤§å°

```python
CONFIG = {
    'batch_size': 32,  # ä»16å¢åŠ åˆ°32ï¼ˆå¦‚æœå†…å­˜å…è®¸ï¼‰
    # æˆ–
    'batch_size': 8,   # å‡å°æ‰¹æ¬¡å¤§å°å¯èƒ½æœ‰åŠ©äºæ³›åŒ–
}
```

---

### 4. æ•°æ®è´¨é‡æ”¹è¿› â­â­â­â­

#### 4.1 æ£€æŸ¥æ•°æ®åˆ†å¸ƒ

ç¡®ä¿æ¯ä¸ªç±»åˆ«æœ‰è¶³å¤Ÿçš„æ ·æœ¬ï¼š

```python
# åœ¨load_gestures()åæ·»åŠ 
def check_data_distribution(self):
    """æ£€æŸ¥æ•°æ®åˆ†å¸ƒ"""
    gesture_counts = {}
    for gesture in self.gestures:
        gesture_dir = os.path.join(self.config['data_dir'], gesture)
        video_count = len([f for f in os.listdir(gesture_dir) if f.endswith('.mp4')])
        gesture_counts[gesture] = video_count
    
    print("\næ•°æ®åˆ†å¸ƒ:")
    for gesture, count in sorted(gesture_counts.items(), key=lambda x: x[1]):
        print(f"  {gesture}: {count} ä¸ªè§†é¢‘")
    
    # æ£€æŸ¥ä¸å¹³è¡¡
    min_count = min(gesture_counts.values())
    max_count = max(gesture_counts.values())
    if max_count / min_count > 5:
        print(f"\nâš ï¸ è­¦å‘Š: æ•°æ®ä¸å¹³è¡¡ï¼Œæ¯”ä¾‹ {max_count/min_count:.1f}:1")
        print("å»ºè®®: ä½¿ç”¨ç±»åˆ«æƒé‡æˆ–è¿‡é‡‡æ ·")
```

#### 4.2 ä½¿ç”¨ç±»åˆ«æƒé‡

å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼š

```python
from sklearn.utils.class_weight import compute_class_weight

# åœ¨train()æ–¹æ³•ä¸­
class_weights = compute_class_weight(
    'balanced',
    classes=np.unique(labels),
    y=labels
)
class_weights = torch.FloatTensor(class_weights).to(self.device)
criterion = nn.CrossEntropyLoss(weight=class_weights)
```

#### 4.3 æ•°æ®æ¸…æ´—

ç§»é™¤è´¨é‡å·®çš„è§†é¢‘ï¼š

```python
# åœ¨extract_features_from_videosä¸­
# æ£€æŸ¥æå–çš„å…³é”®ç‚¹è´¨é‡
if keypoints_seq is not None:
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æœ‰æ•ˆå¸§
    valid_frames = np.sum(np.any(keypoints_seq != 0, axis=1))
    if valid_frames < max_frames * 0.5:  # è‡³å°‘50%çš„å¸§æœ‰æ•ˆ
        continue  # è·³è¿‡è¿™ä¸ªè§†é¢‘
```

---

### 5. ç‰¹å¾å·¥ç¨‹ â­â­â­

#### 5.1 å½’ä¸€åŒ–å…³é”®ç‚¹

```python
def normalize_keypoints(keypoints_seq):
    """å½’ä¸€åŒ–å…³é”®ç‚¹"""
    # ç›¸å¯¹äºèº«ä½“ä¸­å¿ƒç‚¹å½’ä¸€åŒ–
    # æˆ–ä½¿ç”¨æ ‡å‡†åŒ–
    mean = np.mean(keypoints_seq, axis=0, keepdims=True)
    std = np.std(keypoints_seq, axis=0, keepdims=True) + 1e-8
    normalized = (keypoints_seq - mean) / std
    return normalized
```

#### 5.2 æ·»åŠ é€Ÿåº¦ç‰¹å¾

è®¡ç®—å…³é”®ç‚¹çš„é€Ÿåº¦ï¼ˆä¸€é˜¶å¯¼æ•°ï¼‰ï¼š

```python
def add_velocity_features(keypoints_seq):
    """æ·»åŠ é€Ÿåº¦ç‰¹å¾"""
    velocity = np.diff(keypoints_seq, axis=0)
    # åœ¨ç¬¬ä¸€å¸§å‰æ·»åŠ é›¶é€Ÿåº¦
    velocity = np.vstack([np.zeros((1, velocity.shape[1])), velocity])
    # æ‹¼æ¥åŸå§‹ç‰¹å¾å’Œé€Ÿåº¦ç‰¹å¾
    enhanced = np.concatenate([keypoints_seq, velocity], axis=1)
    return enhanced
```

---

### 6. é›†æˆå­¦ä¹  â­â­â­

è®­ç»ƒå¤šä¸ªæ¨¡å‹å¹¶é›†æˆï¼š

```python
# è®­ç»ƒå¤šä¸ªä¸åŒåˆå§‹åŒ–çš„æ¨¡å‹
models = []
for i in range(5):
    model = SignLanguageLSTM(...)
    # è®­ç»ƒæ¨¡å‹
    # ...
    models.append(model)

# é¢„æµ‹æ—¶é›†æˆ
def ensemble_predict(models, keypoints_seq):
    predictions = []
    for model in models:
        pred = model(keypoints_seq)
        predictions.append(torch.softmax(pred, dim=1))
    # å¹³å‡é¢„æµ‹
    ensemble_pred = torch.mean(torch.stack(predictions), dim=0)
    return ensemble_pred
```

---

## ğŸš€ å¿«é€Ÿå®æ–½å»ºè®®ï¼ˆæŒ‰æ•ˆæœæ’åºï¼‰

### ç«‹å³å®æ–½ï¼ˆé«˜æ•ˆæœï¼Œä½éš¾åº¦ï¼‰

1. **å¢åŠ Dropoutç‡** (5åˆ†é’Ÿ)
   ```python
   'dropout_rate': 0.5  # åœ¨CONFIGä¸­
   ```

2. **æ·»åŠ æƒé‡è¡°å‡** (2åˆ†é’Ÿ)
   ```python
   optimizer = optim.Adam(..., weight_decay=0.0001)
   ```

3. **ä½¿ç”¨Early Stopping** (10åˆ†é’Ÿ)
   - é˜²æ­¢è¿‡æ‹Ÿåˆ
   - è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹

4. **å­¦ä¹ ç‡è°ƒåº¦** (5åˆ†é’Ÿ)
   ```python
   scheduler = optim.lr_scheduler.ReduceLROnPlateau(...)
   ```

### ä¸­æœŸå®æ–½ï¼ˆé«˜æ•ˆæœï¼Œä¸­ç­‰éš¾åº¦ï¼‰

5. **æ•°æ®å¢å¼º** (30åˆ†é’Ÿ)
   - æ·»åŠ å™ªå£°ã€æ—¶é—´æ‰­æ›²ç­‰

6. **æ£€æŸ¥æ•°æ®åˆ†å¸ƒ** (15åˆ†é’Ÿ)
   - è¯†åˆ«ä¸å¹³è¡¡ç±»åˆ«
   - ä½¿ç”¨ç±»åˆ«æƒé‡

7. **å½’ä¸€åŒ–ç‰¹å¾** (10åˆ†é’Ÿ)
   - æ ‡å‡†åŒ–å…³é”®ç‚¹

### é•¿æœŸä¼˜åŒ–ï¼ˆä¸­ç­‰æ•ˆæœï¼Œé«˜éš¾åº¦ï¼‰

8. **åŒå‘LSTM** (1å°æ—¶)
   - éœ€è¦è°ƒæ•´æ¨¡å‹æ¶æ„

9. **æ³¨æ„åŠ›æœºåˆ¶** (2å°æ—¶)
   - æ›´å¤æ‚çš„å®ç°

10. **é›†æˆå­¦ä¹ ** (3å°æ—¶+)
    - éœ€è¦è®­ç»ƒå¤šä¸ªæ¨¡å‹

---

## ğŸ“ ä¿®æ”¹åçš„CONFIGç¤ºä¾‹

```python
CONFIG = {
    'data_dir': '/root/autodl-tmp/data',
    'output_dir': '/root/autodl-nus/sign_language_output',
    'train_dataset_dir': '/root/autodl-nus/train_dataset',
    'model_save_path': '/root/autodl-nus/sign_language_model.pth',
    'sequence_length': 30,
    'input_size': 258,
    'hidden_size': 64,  # å¯ä»¥å°è¯•128
    'num_epochs': 200,
    'batch_size': 16,  # å¯ä»¥å°è¯•32æˆ–8
    'learning_rate': 0.001,  # å¯ä»¥å°è¯•0.0005
    'test_size': 0.2,
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    # æ–°å¢é…ç½®
    'dropout_rate': 0.5,  # å¢åŠ æ­£åˆ™åŒ–
    'weight_decay': 0.0001,  # L2æ­£åˆ™åŒ–
    'early_stopping_patience': 20,  # æ—©åœè€å¿ƒå€¼
    'use_data_augmentation': True,  # å¯ç”¨æ•°æ®å¢å¼º
    'normalize_features': True,  # ç‰¹å¾å½’ä¸€åŒ–
}
```

---

## ğŸ¯ é¢„æœŸæ•ˆæœ

å®æ–½ä»¥ä¸Šæ”¹è¿›åï¼Œé¢„æœŸå¯ä»¥è¾¾åˆ°ï¼š

- **æµ‹è¯•å‡†ç¡®ç‡**: 74% â†’ **80-85%**
- **è¿‡æ‹Ÿåˆç¨‹åº¦**: 16% â†’ **<10%**
- **æ³›åŒ–èƒ½åŠ›**: æ˜¾è‘—æå‡

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **ä¸è¦åŒæ—¶å®æ–½æ‰€æœ‰æ”¹è¿›**ï¼Œé€æ­¥æ·»åŠ å¹¶è§‚å¯Ÿæ•ˆæœ
2. **ä¿ç•™åŸå§‹æ¨¡å‹å¤‡ä»½**ï¼Œæ–¹ä¾¿å¯¹æ¯”
3. **è®°å½•æ¯æ¬¡æ”¹è¿›çš„æ•ˆæœ**ï¼Œæ‰¾å‡ºæœ€æœ‰æ•ˆçš„ç­–ç•¥
4. **æ•°æ®è´¨é‡æ˜¯å…³é”®**ï¼Œç¡®ä¿è§†é¢‘è´¨é‡è¶³å¤Ÿå¥½
5. **ç±»åˆ«ä¸å¹³è¡¡**éœ€è¦ç‰¹åˆ«å¤„ç†

---

## ğŸ“Š ç›‘æ§æŒ‡æ ‡

è®­ç»ƒæ—¶å…³æ³¨ï¼š
- è®­ç»ƒå‡†ç¡®ç‡ vs æµ‹è¯•å‡†ç¡®ç‡å·®è·ï¼ˆåº”è¯¥<10%ï¼‰
- å­¦ä¹ ç‡å˜åŒ–
- æŸå¤±å‡½æ•°æ”¶æ•›æƒ…å†µ
- æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡ï¼ˆè¯†åˆ«å›°éš¾ç±»åˆ«ï¼‰

---

**å»ºè®®ä¼˜å…ˆå®æ–½**: Early Stopping + Dropout + æƒé‡è¡°å‡ + å­¦ä¹ ç‡è°ƒåº¦

è¿™äº›æ”¹è¿›ç›¸å¯¹ç®€å•ä½†æ•ˆæœæ˜¾è‘—ï¼Œé¢„è®¡å¯ä»¥å°†æµ‹è¯•å‡†ç¡®ç‡æå‡åˆ°80%ä»¥ä¸Šã€‚

